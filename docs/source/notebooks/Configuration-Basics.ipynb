{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablator has the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.\n",
    "\n",
    "Important configuration classes are presented below. Note that you can either store all these parameters in a yaml config file, or you can play around with classes. You can refer to [these examples]() to see how these 2 cases are done in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters\n",
    "\n",
    "For our framework, configuration is divided into different categories: running configs (either ordinary running configs or parallel training configs), model configs, training configs, optimizer configs, scheduler configs. Details of these configurations is summarized below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunConfig\n",
    "\n",
    "`RunConfig` is used for setting up configuration for the experiment to run, for example, where artifacts of the experiment are stored, the device to be used (gpu, cpu), when to do validation step, when to do logging of training progress. \n",
    "\n",
    "`RunConfig` is passed as an argument when initializing the trainer object:\n",
    "```\n",
    "config = RunConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=CustomModelConfig(),\n",
    "    verbose=\"silent\",\n",
    "    device=\"cpu\",\n",
    "    amp=False,\n",
    "    ...\n",
    ")\n",
    "\n",
    "ablator = ParallelTrainer(wrapper=wrapper, run_config=config)\n",
    "```\n",
    "The table below summarizes parameters that one can use. Note that `RunConfig` requires TrainConfig and ModelConfig to be included when initializing.\n",
    "\n",
    "| Parameter           | Usage                                                                                                                                                                                                                                                                                                             |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| experiment_dir      | location to store experiment artifacts.                                                                                                                                                                                                                                                                           |\n",
    "| random_seed         | random seed.                                                                                                                                                                                                                                                                                                      |\n",
    "| train_config        | training configuration. (check ``TrainConfig`` for more details)                                                                                                                                                                                                                                                  |\n",
    "| model_config        | model configuration. (check ``ModelConfig`` for more details)                                                                                                                                                                                                                                                     |\n",
    "| keep_n_checkpoints  | number of latest checkpoints to keep.                                                                                                                                                                                                                                                                             |\n",
    "| tensorboard         | whether to use tensorboardLogger.                                                                                                                                                                                                                                                                                 |\n",
    "| amp                 | whether to use automatic mixed precision when running on gpu.                                                                                                                                                                                                                                                     |\n",
    "| device              | device to run on.                                                                                                                                                                                                                                                                                                 |\n",
    "| verbose             | verbosity level.                                                                                                                                                                                                                                                                                                  |\n",
    "| eval_subsample      | fraction of the dataset to use for evaluation.                                                                                                                                                                                                                                                                    |\n",
    "| metrics_n_batches   | max number of batches stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                      |\n",
    "| metrics_mb_limit    | max number of megabytes stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                    |\n",
    "| early_stopping_iter | The maximum allowed difference between the current iteration and the last <br />iteration with the best metric before applying early stopping. Early stopping <br />will be triggered if the difference ``(current_itr-best_itr)`` exceeds ``early_stopping_iter``.<br />If set to ``None``, early stopping will not be applied. |\n",
    "| eval_epoch          | The epoch interval between two evaluations.                                                                                                                                                                                                                                                                       |\n",
    "| log_epoch           | The epoch interval between two logging.                                                                                                                                                                                                                                                                           |\n",
    "| init_chkpt          | path to a checkpoint to initialize the model with.                                                                                                                                                                                                                                                                |\n",
    "| warm_up_epochs      | number of epochs marked as warm up epochs.                                                                                                                                                                                                                                                                        |\n",
    "| divergence_factor   | if ``cur_loss > best_loss > divergence_factor``, the model is considered <br />to have diverged.                                                                                                                                                                                                                        |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParallelConfig\n",
    "\n",
    "This configuration is unique to `ablator`. It's a subclass of RunConfig, but adding further arguments that configure parallel training (horizontal scaling of a single experiment). It also helps define the settings of distributed training in the experiment, e.g number of trials, number of trials to run concurrently, the target metrics to optimize, etc.\n",
    "\n",
    "| Parameter             | Usage                                                                                                                                              |\n",
    "|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| total_trials          | total number of trials.                                                                                                                            |\n",
    "| concurrent_trials     | number of trials to run concurrently.                                                                                                              |\n",
    "| search_space          | search space for hyperparameter search,eg. ``{\"train_config.optimizer_config.arguments.lr\": SearchSpace(value_range=[0, 10], value_type=\"int\"),}`` |\n",
    "| optim_metrics         | metrics to optimize, eg. ``{\"val_loss\": \"min\"}``                                                                                                   |\n",
    "| search_algo           | type of search algorithm.                                                                                                                          |\n",
    "| ignore_invalid_params | whether to ignore invalid parameters when sampling.                                                                                                |\n",
    "| remote_config         | remote storage configuration.                                                                                                                      |\n",
    "| gcp_config            | gcp configuration.                                                                                                                                 |\n",
    "| gpu_mb_per_experiment | gpu resource to assign to an experiment.                                                                                                           |\n",
    "| cpus_per_experiment   | cpu resource to assign to an experiment.                                                                                                           |\n",
    "\n",
    "It's worth to mention `search_space`, which is used to define a set (categorical or discrete/continuous) of values for a certain hyperparameter that you want to ablate. Refer to [this]() to learn more about how to use it for ablation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelConfig\n",
    "\n",
    "This configuration can be used to add parameters specific to certain types of models that you're using. One sample use case for this is when we want to try different model size, e.g different number of layers, different activation function, or different dropout ratio, etc. By creating `ModelConfig` class for your model, `ablator` will be able to create search space over them, hence be able to run Hyperparameter optimization.\n",
    "\n",
    "Another example is when we have different models sizes, each comes with different set of pretrained weights. A config class encapsulating these models and their corresponding weight set.\n",
    "\n",
    "There are 2 extra steps you need to do if you create a custom `ModelConfig`. Firstly, since you have defined model parameters to be customized, when creating the model module, you have to pass to its constructor this config, then you can construct the model using this config's parameters. Secondly, as the running config (either RunConfig or ParallelConfig) has an `ModelConfig` attribute, you will need to create a custom running config class as well (decorated with `configclass` decorator), updating `model_config` argument to type `ModelConfig`. Only after this can the running config object be used in ablator launcher.\n",
    "\n",
    "Note that in the model config class, all arguments should be defined as Stateless or Derived data type. These are custom Python annotations to define attributes to which the experiment state is agnostic.\n",
    "- Stateless configuration attributes can be used as a proxy for variables that can take different value assignments between trials or experiments. For example, the learning rate can be set as an independent variable and must be annotated as stateless. Additionally, there are variables that take different values between experiments and trials to which the state is agnostic, for example, a random seed or a directory path between execution environments can be annotated as stateless.\n",
    "- Derived attributes are un-decided at the start of the experiment and do not require a value assignment. Instead, the value is determined by internal experiment processes that can depend on other experimental attributes, such as the dataset. However, given the same initial state, the attribute is expected to result in the same value and is therefore deterministic. For example, the input size used in a modelâ€™s architecture that depends on the dataset will be annotated as Derived during the experiment design phase.\n",
    "\n",
    "The code snippet below shows a concrete example of a simple 1-layer neural network model, with injected configuration for input size, hidden layer dimension, activation function, and dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ablator import RunConfig, ModelConfig, Stateless, Derived, configclass\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MyModelConfig(ModelConfig):\n",
    "    inp_size: Derived[int]\n",
    "    hidden_dim: Stateless[int]\n",
    "    activation: Stateless[str]\n",
    "    dropout: Stateless[float]\n",
    "\n",
    "@configclass\n",
    "class CustomRunConfig(RunConfig):\n",
    "    model_config: MyModelConfig\n",
    "\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, config: MyModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.inp_size, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        if config.activation == \"relu\":\n",
    "            self.activate = nn.ReLU()\n",
    "        elif config.activation == \"elu\":\n",
    "            self.activate = nn.ELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.linear(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.activate(out)\n",
    "\n",
    "        return {\"preds\": out, \"labels\": out}, x.sum().abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainConfig\n",
    "\n",
    "This configuration class defines everything that is related to the main training process of your model, which includes dataset name, batch size, number of epochs, optimizer, scheduler.\n",
    "\n",
    "| Parameter         | Usage                                                                 |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "| dataset           | dataset name. maybe used in custom dataset loader functions.          |\n",
    "| batch_size        | batch size.                                                           |\n",
    "| epochs            | number of epochs to train.                                            |\n",
    "| optimizer_config  | optimizer configuration. (check ``OptimizerConfig`` for more details) |\n",
    "| scheduler_config  | scheduler configuration. (check ``SchedulerConfig`` for more details) |\n",
    "| rand_weights_init | whether to initialize model weights randomly.                         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizerConfig\n",
    "\n",
    "`OptimizerConfig` is a config class that allows user choose the optimizer they wanted, either `Adam`, `SGD`, or `AdamW` optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SchedulerConfig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Configuration\n",
    "\n",
    "Ablator trainer requires a model wrapper and a running config when initializing, after that, experiment can be launched via `trainer.launch()`.\n",
    "```\n",
    "trainer = ProtoTrainer(wrapper=model_wrapper, run_config=run_config)\n",
    "trainer.launch()\n",
    "```\n",
    "This tutorial focuses on helping you define a running configuration `run_config`.\n",
    "\n",
    "Apart from the default parameter values and primitive type parameters (which you can refer to the summary table above to know what value to give them), Running configuration, as shown in table above, requires training configuration and model configuration. So, you must provide it with these configuration objects. Moreover, training configuration also requires optimizer config. So these are the configuration objects that you should create. These configuration objects are just a way to isolate different parameters by functional type in machine learning.\n",
    "\n",
    "Take the code snippet below as an example, `optimizer_config` specifies the optimizer name and arguments, while the `train_config` sets up the dataset, batch size, epochs, and references the optimizer configuration. Next, `config` object combines the `training_config` and `model_config`, along with runtime settings like verbosity and device. Finally, the ProtoTrainer object `ablator` is created with a wrapper object and `config`, and `launch()` method is called to start the experiment. The returned metrics or results are stored in the metrics variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ablator import OptimizerConfig, TrainConfig, RunConfig, ModelConfig\n",
    "\n",
    "optimizer_config = OptimizerConfig(name=\"sgd\", arguments={\"lr\": 0.1})\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    dataset=\"test\",\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=None,\n",
    ")\n",
    "\n",
    "config = RunConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=ModelConfig(),\n",
    "    verbose=\"silent\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways to provide values to the configurations: Named arguments, file-based, or dictionary-based."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named arguments\n",
    "\n",
    "What you saw in the above example is actually named argument method. So you directly create configuration objects and provide config values as you initialize them.\n",
    "\n",
    "Parallel config:\n",
    "\n",
    "Here ModelConfig can be customized to your use case. Note that if you're customizing model configuration as a new config class, you will need to redefine the running configuration's model config to be of the newly created model config class\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File-based\n",
    "\n",
    "Define configs in yaml files\n",
    "\n",
    "Use RunConfig.load(`path`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary based"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chapter, you will learn how to train a model using these configurations in ablator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
