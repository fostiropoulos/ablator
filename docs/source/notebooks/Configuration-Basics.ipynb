{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablator has the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.\n",
    "\n",
    "Important configuration classes are presented below. Note that you can either store all these parameters in a yaml config file, or you can play around with classes. You can refer to [these examples]() to see how these 2 cases are done in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration parameters\n",
    "\n",
    "For our framework, configuration is organized into different categories: running configs (either for training a single model or training multiple models in parallel), model configs, training configs, optimizer configs, scheduler configs. Most of these will be used together in order for `ablator` to work seamlessly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunConfig\n",
    "\n",
    "`RunConfig` is used for setting up configuration for the experiment to run, for example, where to store experiment artifacts, the device to be used (gpu, cpu), when to do validation step, when to do progress logging. \n",
    "\n",
    "The table below summarizes parameters that one can use and need to define. Note that `RunConfig` requires TrainConfig and ModelConfig to be included during initialization. Learn more about these in the next sections of this tutorial.\n",
    "\n",
    "| Parameter           | Usage                                                                                                                                                                                                                                                                                                             |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| experiment_dir      | location to store experiment artifacts.                                                                                                                                                                                                                                                                           |\n",
    "| random_seed         | random seed.                                                                                                                                                                                                                                                                                                      |\n",
    "| train_config        | training configuration. (check ``TrainConfig`` for more details)                                                                                                                                                                                                                                                  |\n",
    "| model_config        | model configuration. (check ``ModelConfig`` for more details)                                                                                                                                                                                                                                                     |\n",
    "| keep_n_checkpoints  | number of latest checkpoints to keep.                                                                                                                                                                                                                                                                             |\n",
    "| tensorboard         | whether to use tensorboardLogger.                                                                                                                                                                                                                                                                                 |\n",
    "| amp                 | whether to use automatic mixed precision when running on gpu.                                                                                                                                                                                                                                                     |\n",
    "| device              | device to run on.                                                                                                                                                                                                                                                                                                 |\n",
    "| verbose             | verbosity level.                                                                                                                                                                                                                                                                                                  |\n",
    "| eval_subsample      | fraction of the dataset to use for evaluation.                                                                                                                                                                                                                                                                    |\n",
    "| metrics_n_batches   | max number of batches stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                      |\n",
    "| metrics_mb_limit    | max number of megabytes stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                    |\n",
    "| early_stopping_iter | The maximum allowed difference between the current iteration and the last <br />iteration with the best metric before applying early stopping. Early stopping <br />will be triggered if the difference ``(current_itr-best_itr)`` exceeds ``early_stopping_iter``.<br />If set to ``None``, early stopping will not be applied. |\n",
    "| eval_epoch          | The epoch interval between two evaluations.                                                                                                                                                                                                                                                                       |\n",
    "| log_epoch           | The epoch interval between two logging.                                                                                                                                                                                                                                                                           |\n",
    "| init_chkpt          | path to a checkpoint to initialize the model with.                                                                                                                                                                                                                                                                |\n",
    "| warm_up_epochs      | number of epochs marked as warm up epochs.                                                                                                                                                                                                                                                                        |\n",
    "| divergence_factor   | if ``cur_loss > best_loss > divergence_factor``, the model is considered <br />to have diverged.                                                                                                                                                                                                                        |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParallelConfig\n",
    "\n",
    "`ParallelConfig` is a subclass of `RunConfig`. It extends the functionality by introducing additional arguments to configure parallel training, enabling horizontal scaling of a single experiment. It helps define the setting of distributed training in the experiment, such as specifying the number of trials, the maximum number of trials to run concurrently, the target metrics to optimize, and more.\n",
    "\n",
    "| Parameter             | Usage                                                                                                                                              |\n",
    "|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| total_trials          | total number of trials.                                                                                                                            |\n",
    "| concurrent_trials     | number of trials to run concurrently.                                                                                                              |\n",
    "| search_space          | search space for hyperparameter search,eg. ``{\"train_config.optimizer_config.arguments.lr\": SearchSpace(value_range=[0, 10], value_type=\"int\"),}`` |\n",
    "| optim_metrics         | metrics to optimize, eg. ``{\"val_loss\": \"min\"}``                                                                                                   |\n",
    "| search_algo           | type of search algorithm.                                                                                                                          |\n",
    "| ignore_invalid_params | whether to ignore invalid parameters when sampling.                                                                                                |\n",
    "| remote_config         | remote storage configuration.                                                                                                                      |\n",
    "| gcp_config            | gcp configuration.                                                                                                                                 |\n",
    "| gpu_mb_per_experiment | gpu resource to assign to an experiment.                                                                                                           |\n",
    "| cpus_per_experiment   | cpu resource to assign to an experiment.                                                                                                           |\n",
    "\n",
    "It's worth mentioning `search_space`, which is used to define a set of categorical or discrete/continuous values for a certain hyperparameter that you want to ablate. Refer to [this]() to learn more about how to use it for ablation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelConfig\n",
    "\n",
    "This configuration can be used to add parameters specific to the model you're using. A sample use case for this is when you want to try different model sizes, number of layers, activation functions, etc. By creating `ModelConfig` class for your model and include these parameters, `ablator` will be able to create search space over them, hence able to run Hyperparameter optimization.\n",
    "\n",
    "There are 2 extra steps you need to do if you create a custom config from `ModelConfig`. Firstly, since you have defined model parameters to be customized, when creating the model module, you have to pass to its constructor this custom config, then construct the model using this its parameters. Secondly, as the running config (`RunConfig` or `ParallelConfig`) has `model_config` attribute, you will need to create a custom running config class as well (decorated with `configclass` decorator), updating `model_config` argument to proper type. Only after this can the running config object be used in `ablator` launcher.\n",
    "\n",
    "Note that in the model config class, all arguments should be defined as Stateless or Derived data type. These are custom Python annotations to define attributes to which the experiment state is agnostic.\n",
    "\n",
    "- Stateless configuration attributes can be used as a proxy for variables that can take different value assignments between trials or experiments. For example, the learning rate can be set as an independent variable and must be annotated as stateless. Additionally, there are variables that take different values between experiments and trials to which the state is agnostic, for example, a random seed or a directory path between execution environments can be annotated as stateless.\n",
    "\n",
    "- Derived attributes are un-decided at the start of the experiment and do not require a value assignment. Instead, the value is determined by internal experiment processes that can depend on other experimental attributes, such as the dataset. However, given the same initial state, the attribute is expected to result in the same value and is therefore deterministic. For example, the input size used in a modelâ€™s architecture that depends on the dataset will be annotated as Derived during the experiment design phase.\n",
    "\n",
    "Here's a concrete example of a simple 1-layer neural network model, with injected configuration for input size, hidden layer dimension, activation function, and dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ablator import RunConfig, ModelConfig, Stateless, Derived, configclass\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MyModelConfig(ModelConfig):\n",
    "    inp_size: Derived[int]\n",
    "    hidden_dim: Stateless[int]\n",
    "    activation: Stateless[str]\n",
    "    dropout: Stateless[float]\n",
    "\n",
    "@configclass\n",
    "class CustomRunConfig(RunConfig):\n",
    "    model_config: MyModelConfig\n",
    "\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, config: MyModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.inp_size, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        if config.activation == \"relu\":\n",
    "            self.activate = nn.ReLU()\n",
    "        elif config.activation == \"elu\":\n",
    "            self.activate = nn.ELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.linear(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.activate(out)\n",
    "\n",
    "        return {\"preds\": out, \"labels\": out}, x.sum().abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainConfig\n",
    "\n",
    "This configuration class defines everything that is related to the main training process of your model, which includes dataset name, batch size, number of epochs, optimizer, scheduler. 2 important attributes to metion are `optimizer_config` and `scheduler_config`. As the names suggest, they configure the optimizer and scheduler to be used in the training process.\n",
    "\n",
    "| Parameter         | Usage                                                                 |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "| dataset           | dataset name. maybe used in custom dataset loader functions.          |\n",
    "| batch_size        | batch size.                                                           |\n",
    "| epochs            | number of epochs to train.                                            |\n",
    "| optimizer_config  | optimizer configuration. (check ``OptimizerConfig`` for more details) |\n",
    "| scheduler_config  | scheduler configuration. (check ``SchedulerConfig`` for more details) |\n",
    "| rand_weights_init | whether to initialize model weights randomly.                         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizerConfig and SchedulerConfig\n",
    "\n",
    "`OptimizerConfig` is a config class that allows user choose the optimizer they wanted. Currently, we supports SGD optimizer, Adam optimizer, and AdamW optimizer.\n",
    "\n",
    "`SchedulerConfig`, on the other hand, can be used for scheduling learning rate updates in the training process.\n",
    "\n",
    "Both of these config classes have similar arguments:\n",
    "\n",
    "| Parameter | Usage                                                                                                                                                                       |\n",
    "|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| name      | The type of the scheduler or optimizer, this can be any in ``['None', 'step', 'cycle', 'plateau']``  for schedulers and in ``['sgd', 'adam', 'adamw']`` for optimizers. |\n",
    "| arguments | The arguments for the scheduler or optimizer, specific to a certain type of scheduler or scheduler.                                                                         |\n",
    "\n",
    "The table below shows how arguments can be defined for each type of scheduler:\n",
    "\n",
    "| Scheduler type | Arguments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "|----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cycle          | `max_lr` : Upper learning rate boundaries in the cycle.  `total_steps` : The total number of steps to run the scheduler in a cycle.  `step_when` : The step type at which the `scheduler.step()` should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.             |\n",
    "| plataeu        | `patience` : Number of epochs with no improvement after which learning rate will be reduced.  `min_lr` : A lower bound on the learning rate. `mode` : One of ``'min'``, ``'max'``, or ``'auto'``, which defines the  direction of optimization, so as to adjust the learning rate accordingly, i.e when a certain metric ceases improving.  `factor` : Factor by which the learning rate will be reduced. ``new_lr = lr * factor``.  `threshold` : Threshold for measuring the new optimum, to only focus on significant changes.  `verbose` : If ``True``, prints a message to ``stdout`` for each update.  `step_when` : The step type at which the scheduler should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.  |\n",
    "| step           | `step_size` : Period of learning rate decay, by default 1.  `gamma` : Multiplicative factor of learning rate decay, by default 0.99.  `step_when` : The step type at which the scheduler should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "\n",
    "The table below shows how arguments can be defined for each type of optimzer:\n",
    "\n",
    "| Optimizer type | Arguments                                                                                                                                                                                                                                                                           |\n",
    "|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| sgd            | `weight_decay` : Weight decay rate<br> `momentum` : Momentum factor<br>                                                                                                                                                                                                           |\n",
    "| adam           | `betas` : Coefficients for computing running averages of gradient and its square (default is ``(0.5, 0.9)``).<br/> `weight_decay` : Weight decay rate (default is ``0.0``).<br/>                                                                                                    |\n",
    "| adamw          | `betas` : Coefficients for computing running averages of gradient and its square (default is ``(0.9, 0.999)``).<br/> `eps` : Term added to the denominator to improve numerical stability (default is ``1e-8``).<br/> `weight_decay` : Weight decay rate (default is ``0.0``).<br/> |\n",
    "\n",
    "The following code snippet describes how to initialize these objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ablator import OptimizerConfig, SchedulerConfig\n",
    "\n",
    "optimizer_config = OptimizerConfig(name=\"sgd\", arguments={\"lr\": 0.1})\n",
    "scheduler_config = SchedulerConfig(name=\"cycle\", arguments={\"max_lr\": 0.5, \"total_steps\": 50})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine everything. Ablator trainer requires a model wrapper and a running config when initializing, after that, experiment can be launched via `trainer.launch()`. This tutorial focuses on helping you define the running configuration `run_config`.\n",
    "\n",
    "Take the code snippet below as an example, `train_config` sets up the dataset, batch size, epochs, and references the optimizer configuration. Next, `config` object combines the `training_config` and `model_config`, along with runtime settings like verbosity and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ablator import TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    dataset=\"test\",\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=scheduler_config,\n",
    ")\n",
    "\n",
    "config = CustomRunConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=MyModelConfig(),\n",
    "    verbose=\"silent\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the configuration created, we are half-way to running ablation experiment with the ablator trainer.\n",
    "```\n",
    "trainer = ParallelTrainer(wrapper=model_wrapper, run_config=run_config)\n",
    "trainer.launch()\n",
    "```\n",
    "In the next chapter, you will learn how to create the model wrapper, the other half that's left. We will start with training a single model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different methods to define running configurations\n",
    "\n",
    "There are 3 ways to provide values to the configurations: named arguments, file-based, or dictionary-based. All examples from the previous sections are actually the named arguments method. Now let's look at how file based method and dictionary based method work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File-based\n",
    "\n",
    "File based configuration is a way for you to create simple configuration files, passing configuration values to a single `yaml` file. After that, based on the type of running configuration you want, you can use `RunConfigClass.load(path/to/yaml/file)` method to create configuration with values provided in the config file.\n",
    "\n",
    "To write these config files, simply follow `key : value` syntax (each pair on a single line). The following example shows what a config yaml file looks like. We will name it `config.yaml`:\n",
    "```\n",
    "experiment_dir: \"/tmp/dir\"\n",
    "train_config:\n",
    "  dataset: cifar10\n",
    "  optimizer_config:\n",
    "    name: adam\n",
    "    arguments:\n",
    "      lr: 0.01\n",
    "      weight_decay: 0.0\n",
    "  batch_size: 128\n",
    "  epochs: 10\n",
    "  scheduler_config: null\n",
    "  rand_weights_init: true\n",
    "model_config:\n",
    "  inp_size: 50\n",
    "  hidden_dim: 100\n",
    "  activation: \"relu\"\n",
    "  dropout: 0.15\n",
    "metrics_n_batches: 400\n",
    "```\n",
    "We can see that the outermost arguments are from `RunConfig`. Also note how `train_config`, which corresponds to `TrainConfig` object in the running config, has its arguments defined 1 level below (indented). Therefore, the first rule to follow is that the arguments to use are from the running config class, either `RunConfig` or `ParallelConfig`, so make sure you use the right set of arguments. The second rule is that any arguments that is another config class should be indented 1 level from its parent config class.\n",
    "\n",
    "Now in your code, only 1 single line of code is required to load these values to create the config object:\n",
    "```\n",
    "config = CustomRunConfig.load(\"path/to/yaml/file\")\n",
    "```\n",
    "\n",
    "Note that since we created a custom running configuration class `CustomRunConfig` that is tied to the custom model config in the previous sections, we used `CustomRunConfig.load(\"path/to/yaml/file\")` to load configuration from file. Otherwise, if you're not creating any subclasses, `RunConfig.load(\"path\")` or `ParallelConfig.load(\"path\")` also works."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary based\n",
    "\n",
    "Create dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
