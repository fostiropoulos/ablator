{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will familiarize you with the parallel experiment's output folder structure, where some files can be viewed directly to quickly draw conclusions about the experiment's results, some can be visualized in tensorboard, and some can be used to create custom visualizations with the [`ablator.analysis module`](../analysis.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment output folder structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiments from [Hyperparameter Optimization](./HPO-tutorial.ipynb) tutorial, you can inspect the results saved in the following folder: `/tmp/experiments` (specified in the configurations `parallel_config.experiment_dir`). The directory follows the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- tmp/experiments\n",
    "    - experiment_<experiment_id>\n",
    "        - <trial1_id>\n",
    "          - best_checkpoints/\n",
    "          - checkpoints/\n",
    "          - dashboard/\n",
    "          - config.yaml\n",
    "          - metadata.json\n",
    "          - results.json\n",
    "          - train.log\n",
    "        - <trial2_id>\n",
    "        - <trial3_id>\n",
    "        - ...\n",
    "        - <experiment_id>_optuna.db\n",
    "        - <experiment_id>_state.db\n",
    "        - master_config.yaml\n",
    "        - mp.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two levels of directories: one for the ablation experiment and one for the trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `experiment_<experiment_id>` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directory contains:\n",
    "\n",
    "  - `<experiment_id>_optuna.db`: for each trial, after finishing training, its metrics (those to be optimized) will be recorded to the experiment state database (via `optuna_study.tell()`). Optuna will use this as a base to explore the search space based on the results so far. The content of this database is out of this tutorial's scope since it's used by `optuna` to perform hyperparameters exploration.\n",
    "\n",
    "  - `<experiment_id>_state.db`: for each trial, after finishing training, its metrics (those to be optimized), configuration, and training state (RUNNING, WAITING, etc.), will be added to the experiment state database.\n",
    "  \n",
    "  - `master_config.yaml`: the overall configurations of the experiment. Note that configurations for hyperparameters will be changed in each trial's `config.yaml` file that's specific to that trial only.\n",
    "  \n",
    "  - `mp.log`: console infomation during the running experiment. This gives information about the trials that are running in parallel, how many that are running, and how many that are terminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trial directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding to each trial, a folder named by the trial's id will be created, each contains:\n",
    "\n",
    "- `train.log`: this log reports metrics from training and evaluation of the training process of the trial. These metrics include static (e.g., best loss value, the current iteration, current epoch, best iteration so far, total steps, current learning rate) and moving average metrics (e.g., training loss, validation loss, user-defined metrics like f1 score, precision, etc.).\n",
    "\n",
    "- `results.json`: helps keep track of the running trial. This includes a JSON object for each of the epochs. This is where all metrics in the `train.log` are stored as JSON objects.\n",
    "\n",
    "```\n",
    "{\n",
    "\"train_loss\": 0.5761058599829674,\n",
    "\"val_loss\": NaN,\n",
    "\"train_accuracy\": 0.855390625,\n",
    "\"val_accuracy\": NaN,\n",
    "\"best_iteration\": 0,\n",
    "\"best_loss\": Infinity,\n",
    "\"current_epoch\": 1,\n",
    "\"current_iteration\": 1875,\n",
    "\"epochs\": 10,\n",
    "\"learning_rate\": 0.008175538431326259,\n",
    "\"total_steps\": 18750\n",
    "},\n",
    "{\n",
    "\"train_loss\": 0.3937998796661695,\n",
    "\"val_loss\": 0.42723633049014276,\n",
    "\"train_accuracy\": 0.8599609374999999,\n",
    "\"val_accuracy\": 0.8481,\n",
    "\"best_iteration\": 1875,\n",
    "\"best_loss\": 0.42723633049014276,\n",
    "\"current_epoch\": 2,\n",
    "\"current_iteration\": 3750,\n",
    "\"epochs\": 10,\n",
    "\"learning_rate\": 0.008175538431326259,\n",
    "\"total_steps\": 18750\n",
    "},\n",
    "...\n",
    "{\n",
    "\"train_loss\": 0.3794497859179974,\n",
    "\"val_loss\": 0.4047809976875782,\n",
    "\"train_accuracy\": 0.86644921875,\n",
    "\"val_accuracy\": 0.8576999999999999,\n",
    "\"best_iteration\": 15000,\n",
    "\"best_loss\": 0.3985074917415778,\n",
    "\"current_epoch\": 10,\n",
    "\"current_iteration\": 18750,\n",
    "\"epochs\": 10,\n",
    "\"learning_rate\": 0.008175538431326259,\n",
    "\"total_steps\": 18750\n",
    "}\n",
    "```\n",
    "\n",
    "As you can observe from this sample `results.json` file from HPO tutorial, there are 10 JSON objects, each representing metrics of one epoch. `best_iteration` and `best_loss` values give us information about the best-performing iteration.\n",
    "\n",
    "- `config.yaml`: configuration details that are specific to the trial.\n",
    "\n",
    "- `checkpoints/`: this directory stores checkpoints that are saved during the training process. Each checkpoint includes the model state, the optimizer (and/or scheduler) state, and all the metrics this model has generated. Config parameter `keep_n_checkpoints` from the running configuration controls the number of checkpoints kept in this folder. You can play around with the checkpoint files by loading the `.pt` file with pytorch. For example, you can load the model parameters and optimizer state with the following code:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "checkpoint_path = \"tmp/experiments/experiment_<experiment_id>/<trial1_id>/checkpoints/<ckpt_name>.pt\"\n",
    "torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "```\n",
    "```\n",
    "{'run_config': {'model_config': {'num_filter1': 47,\n",
    "'num_filter2': 74,\n",
    "'activation': 'relu'},\n",
    "'experiment_dir': '/content/experiments/experiment_5ade_3be2',\n",
    "'random_seed': 42,\n",
    "'train_config': {...},\n",
    "'scheduler_config': None,\n",
    "'rand_weights_init': True}\n",
    "'search_space': {...},\n",
    "'optim_metrics': {'val_loss': <Optim.min: 'min'>},,\n",
    "'metrics': {'train_loss': 0.37417657624085743,\n",
    "  'val_loss': 0.3985074917415778,\n",
    "  'train_accuracy': 0.8660498046875,},\n",
    "'model': OrderedDict([('model.conv1.weight',\n",
    "              tensor([[[[-3.0586e-01,  1.4900e-01, -4.6661e-01],\n",
    "                        [ 2.4052e-01, -5.5201e-01, -2.5857e-01],\n",
    "                        [-2.9385e-01, -1.5303e+00, -9.4572e-01]]],\n",
    "                      ])),...\n",
    "                      ]),\n",
    "'optimizer': {'state': {0: {'step': tensor(14992.),...}}}\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "- `best_checkpoints`: this directory stores the checkpoints that perform the best.\n",
    "\n",
    "- `dashboard/`: ablator automatically write metrics to this directory, and you can use Tensorboard to visualize how metrics oscillate while training:\n",
    "  - Install `tensorboard` and load using `%load_ext tensorboard` if using notebook\n",
    "  - Then run `%tensorboard --logdir /tmp/experiments/experiment_<experiment_id> --port [port]`. E.g.:\n",
    "    ```\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir /tmp/experiments/experiment_5ade_3be2 --port 6008\n",
    "    ```\n",
    "![TensorBoard-Output](./Images/tensorboard-output.jpg)\n",
    "\n",
    "- `metadata.json`: keeps track of the training progress: log iteration (specifies the latest iteration that logs the results to files), checkpoint iteration specifies which iteration a checkpoint has been saved and which iteration is the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we quickly look through the experiment output directory and discover how we can use some of them to help study the experiment. In the following tutorial, we will use the [`ablator.analysis module`](../analysis.rst) to visualize these results to get some conclusion on the ablation study of the experiment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
