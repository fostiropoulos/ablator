{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search space for different types of optimizers and schedulers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different optimizers have different update rules and behavior, and they may perform better or worse depending on the specific dataset and model architecture. Hence, trying out different optimizers and learning rate schedulers can be beneficial for HPO.\n",
    "\n",
    "- To work with different optimizers effectively in the ablator, it is necessary to create custom `OptimizerConfig` objects that can handle passing either torch-defined or custom optimizers to the ablator.\n",
    "\n",
    "- This is similar to the schedulers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import the necessary libraries and modules:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau, StepLR\n",
    "\n",
    "from ablator import configclass, ConfigBase\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search space for different optimizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function called `create_optimizer` that creates an optimizer object based on the given inputs (optimizer name, model to optimize, and learning rate). In this example, we support three optimizers: Adam, AdamW, and SGD (but we can also pass our custom-defined optimizers). In specific, the function does the following:\n",
    "\n",
    "- Creates a list of model parameters `parameter_groups` from the model module `model.named_parameters()`.\n",
    "\n",
    "- Defines dictionaries with specific parameters for each optimizer.\n",
    "\n",
    "- Create the optimizer using the model parameters, learning rate, and the defined dictionaries for each optimizer parameters.\n",
    "\n",
    "Returns the optimizer object."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def create_optimizer(optimizer_name: str, model: nn.Module, lr: float):\n",
    "\n",
    "    parameter_groups = [v for k, v in model.named_parameters()]\n",
    "\n",
    "    adamw_parameters = {\n",
    "      \"betas\": (0.0, 0.1),\n",
    "      \"eps\": 0.001,\n",
    "      \"weight_decay\": 0.1\n",
    "    }\n",
    "    adam_parameters = {\n",
    "      \"betas\" : (0.0, 0.1),\n",
    "      \"weight_decay\": 0.0\n",
    "    }\n",
    "    sgd_parameters = {\n",
    "      \"momentum\": 0.9,\n",
    "      \"weight_decay\": 0.1\n",
    "    }\n",
    "\n",
    "    Optimizer = None\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        Optimizer = optim.Adam(parameter_groups, lr = lr, **adam_parameters)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        Optimizer = optim.AdamW(parameter_groups, lr = lr, **adamw_parameters)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        Optimizer = optim.SGD(parameter_groups, lr = lr, **sgd_parameters)\n",
    "\n",
    "\n",
    "    return Optimizer\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create an Optimizer configuration `CustomOptimizerConfig`. Internally, Ablator requires that the optimizer config has function `make_optimizer` with input as a model module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@configclass\n",
    "class CustomOptimizerConfig(ConfigBase):\n",
    "    name: Literal[\"adam\", \"adamw\", \"sgd\"] = \"adam\"\n",
    "    lr: float = 0.001\n",
    "\n",
    "    def make_optimizer(self, model: nn.Module):\n",
    "        return create_optimizer(self.name, model, self.lr)\n",
    "\n",
    "optimizer_config = CustomOptimizerConfig(name = \"adam\", lr = 0.001)\n",
    "```\n",
    "\n",
    "- Here the configuration attribute `name` will be used in the search space, and we're allowing search space to be from the set of values `[\"adam\", \"adamw\", \"sgd\"]`.\n",
    "\n",
    "- Inside `make_optimizer`, we call `create_optimizer` with the model, the name and lr attributes of the config object, and this function will return the corresponding optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search space for different schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function called `create_scheduler` that creates a scheduler object based on the given inputs (scheduler name, the model to optimize, the optimizer used). In this example, we support three schedulers: StepLR, OneCycleLR, and ReduceLROnPlateau (but we can also pass our custom-defined schedulers). In specific, the function does the following:\n",
    "\n",
    "- Defines dictionaries with specific parameters for each scheduler.\n",
    "\n",
    "- Create the scheduler using the optimizer and the defined dictionaries for each scheduler parameters.\n",
    "\n",
    "- Return the scheduler object.\n",
    "\n",
    "We also define a second function called scheduler_arguments that returns the arguments of the scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def create_scheduler(scheduler_name: str, model: nn.Module, optimizer: torch.optim):\n",
    "\n",
    "  parameters = scheduler_arguments(scheduler_name)\n",
    "\n",
    "  Scheduler = None\n",
    "  \n",
    "  if scheduler_name == \"step\":\n",
    "    Scheduler = StepLR(optimizer, **parameters)\n",
    "  elif scheduler_name == \"cycle\":\n",
    "    Scheduler = OneCycleLR(optimizer, **parameters)\n",
    "  elif scheduler_name == \"plateau\":\n",
    "    Scheduler = ReduceLROnPlateau(optimizer, **parameters)\n",
    "    \n",
    "  return Scheduler\n",
    "\n",
    "def scheduler_arguments(scheduler_name):\n",
    "  if scheduler_name == \"step\":\n",
    "    return {\n",
    "      \"step_size\" : 1,\n",
    "      \"gamma\" : 0.99\n",
    "    }\n",
    "  elif scheduler_name == \"cycle\":\n",
    "    return {\n",
    "      \"patience\":  10,\n",
    "      \"min_lr\":  1e-5,\n",
    "      \"mode\":  \"min\",\n",
    "      \"factor\":   0.0,\n",
    "      \"threshold\":  1e-4\n",
    "    }\n",
    "  elif scheduler_name == \"plateau\":\n",
    "    return {\n",
    "      \"max_lr\": 1e-3,\n",
    "      \"total_steps\": 10\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we also create a custom config `CustomSchedulerConfig`, defining the required method `make_scheduler` with shceduler name, the model, and the optimizer as inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@configclass\n",
    "class CustomSchedulerConfig(SchedulerConfig):\n",
    "    name: Literal[\"step\", \"cycle\", \"plateau\"] = \"step\"\n",
    "\n",
    "    def __init__(self, name, arguments=None):\n",
    "        self.arguments = scheduler_arguments(self.name)\n",
    "        super(CustomSchedulerConfig, self).__init__(name=self.name, arguments=self.arguments)\n",
    "\n",
    "    def make_scheduler(self, model: torch.nn.Module, optimizer: torch.optim):\n",
    "        return create_scheduler(self.name, model, optimizer)\n",
    "\n",
    "scheduler_config = CustomSchedulerConfig(name = \"step\")\n",
    "```\n",
    "- Here the configuration attribute `name` will be used in the search space, and we're allowing search space to be from the set of values `[\"step\", \"cycle\", \"plateau\"]`.\n",
    "\n",
    "- We overwrite the constructor, creating an attribute called `arguments`, which is internally accessed by ablator and pass it to the parent class constructor.\n",
    "\n",
    "- Inside `make_scheduler`, we call `create_scheduler` with the optimizer, the name and lr attributes of the config object, and this function will return the corresponding scheduler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Remember to redefine the `TrainConfig` config class, hence the `ParallelConfig`, before creating the training config to pass in the optimizer and scheduler config objects. E.g:\n",
    "\n",
    "```python\n",
    "@configclass\n",
    "class CustomTrainConfig(TrainConfig):\n",
    "  optimizer_config: CustomOptimizerConfig\n",
    "  scheduler_config: CustomSchedulerConfig\n",
    "\n",
    "@configclass\n",
    "class CustomParallelConfig(ParallelConfig):\n",
    "  model_config: CustomModelConfig\n",
    "  train_config: CustomTrainConfig\n",
    "\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create search space for optimizers and schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try out different optimizers and schedulers by providing a search space to the ablator.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_space = {\n",
    "    \"train_config.optimizer_config.lr\": SearchSpace(value_range = [0.001, 0.01], value_type = 'float'),\n",
    "    \"train_config.optimizer_config.name\": SearchSpace(categorical_values = [\"adam\", \"sgd\", \"adamw\"]),\n",
    "    \"train_config.scheduler_config.name\": SearchSpace(categorical_values = [\"step\", \"cycle\", \"plateau\"])\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note:\n",
    "\n",
    "In the default optimizer config, providing the name of the optimizer in the config will create an object of the associated optimizer class. Simply changing the name in the search space will result in a mismatch in the class type, causing an error. Hence, we have to define custom configs in this way.\n",
    "\n",
    "One benefit this method offers is that we can define our custom optimizers or schedulers as a class and pass them to their respective configs for the ablator to manage training.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Finally, with this, we can now test different optimizers and schedulers for our model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
